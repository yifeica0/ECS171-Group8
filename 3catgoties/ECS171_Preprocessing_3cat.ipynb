{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C99Hul6-Lkf0"
      },
      "source": [
        "Reference: https://www.geeksforgeeks.org/nlp/amazon-product-reviews-sentiment-analysis-in-python/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BkWPBlb6jhN",
        "outputId": "a26ca534-697e-4fa9-e529-948758ef8459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.24.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.24.0->datasets) (8.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jll8MTHuKXtu",
        "outputId": "57c2bc3b-5006-44f8-f02f-edbe1fceca09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import random\n",
        "import nltk\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize once for all processes\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDFXNeaYKdEt"
      },
      "outputs": [],
      "source": [
        "categories = [\n",
        "    \"All_Beauty\",\n",
        "    \"Amazon_Fashion\",\n",
        "    \"Appliances\",\n",
        "    \"Arts_Crafts_and_Sewing\",\n",
        "    \"Automotive\",\n",
        "    \"Baby_Products\",\n",
        "    \"Beauty_and_Personal_Care\",\n",
        "    \"Books\",\n",
        "    \"CDs_and_Vinyl\",\n",
        "    \"Cell_Phones_and_Accessories\",\n",
        "    \"Clothing_Shoes_and_Jewelry\",\n",
        "    \"Digital_Music\",\n",
        "    \"Electronics\",\n",
        "    \"Gift_Cards\",\n",
        "    \"Grocery_and_Gourmet_Food\",\n",
        "    \"Handmade_Products\",\n",
        "    \"Health_and_Household\",\n",
        "    \"Health_and_Personal_Care\",\n",
        "    \"Home_and_Kitchen\",\n",
        "    \"Industrial_and_Scientific\",\n",
        "    \"Kindle_Store\",\n",
        "    \"Magazine_Subscriptions\",\n",
        "    \"Movies_and_TV\",\n",
        "    \"Musical_Instruments\",\n",
        "    \"Office_Products\",\n",
        "    \"Patio_Lawn_and_Garden\",\n",
        "    \"Pet_Supplies\",\n",
        "    \"Software\",\n",
        "    \"Sports_and_Outdoors\",\n",
        "    \"Subscription_Boxes\",\n",
        "    \"Tools_and_Home_Improvement\",\n",
        "    \"Toys_and_Games\",\n",
        "    \"Video_Games\",\n",
        "    \"Unknown\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmP6qZTGNDWn"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text data\n",
        "    - Convert to lowercase\n",
        "    - Tokenize into words\n",
        "    - Remove stopwords, punctuation, non-alphabetic tokens\n",
        "    - Lemmatize words\n",
        "    \"\"\"\n",
        "    text = text.lower().strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(w)\n",
        "        for w in tokens\n",
        "        if w not in stop_words and w not in string.punctuation and w.isalpha()\n",
        "    ]\n",
        "    return cleaned_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-VvhMtJK8JO"
      },
      "outputs": [],
      "source": [
        "# load data of all categories; store in the form that includes: category, reviews and sentiment level (1-5)\n",
        "\n",
        "# we load 5000 reviews from each category, then sample 600 reviews with balanced sentiment distribution\n",
        "# rating1-2: 200 (sentiment 2), rating3: 200 (sentiment 1), rating4-5: 200 (sentiment 0)\n",
        "def process_category(category, reservoir_size=5000):\n",
        "    \"\"\" return a dictionary with the structure: {\"category\":[\n",
        "      {\"category\":[name],\"tokens\":[......], \"rating\":[1], \"sentiment\": 2},{...}\n",
        "      ]}\"\"\"\n",
        "    try:\n",
        "\n",
        "      # load the data from hugging face\n",
        "        data_url = f\"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/review_categories/{category}.jsonl\"\n",
        "\n",
        "        dataset = load_dataset(\"json\", data_files=data_url, split=\"train\", streaming=True)\n",
        "\n",
        "        raw_rows = []\n",
        "        for i, row in enumerate(dataset):\n",
        "            if i >= reservoir_size:\n",
        "                break\n",
        "\n",
        "            raw_rows.append({\n",
        "                'text': row.get('text', ''),\n",
        "                'rating': row.get('rating', row.get('overall', None)),\n",
        "                'title': row.get('title', ''),\n",
        "                'images': row.get('images', []),\n",
        "                'verified_purchase': row.get('verified_purchase', False),\n",
        "                'asin': row.get('asin', ''),\n",
        "                'parent_asin': row.get('parent_asin', ''),\n",
        "                'user_id': row.get('user_id', ''),\n",
        "                'timestamp': row.get('timestamp', None),\n",
        "                'helpful_vote': row.get('helpful_vote', 0)\n",
        "            })\n",
        "\n",
        "        # set a seed for randomization\n",
        "        random.seed(42)\n",
        "        \n",
        "        # Group raw_rows by sentiment groups: 1-2 (positive=2), 3 (neutral=1), 4-5 (negative=0)\n",
        "        sentiment_groups = {\n",
        "            2: [],  # rating 1-2, positive\n",
        "            1: [],  # rating 3, neutral\n",
        "            0: []   # rating 4-5, negative\n",
        "        }\n",
        "        \n",
        "        for row in raw_rows:\n",
        "            rating = row['rating']\n",
        "            if rating in [1, 2]:\n",
        "                sentiment_groups[2].append(row)\n",
        "            elif rating == 3:\n",
        "                sentiment_groups[1].append(row)\n",
        "            elif rating in [4, 5]:\n",
        "                sentiment_groups[0].append(row)\n",
        "        \n",
        "        # Sample 200 from each sentiment group\n",
        "        sampled_rows = []\n",
        "        sample_per_group = 200\n",
        "        \n",
        "        for sentiment_label, group in sentiment_groups.items():\n",
        "            if len(group) > 0:\n",
        "                sample = random.sample(group, min(sample_per_group, len(group)))\n",
        "                sampled_rows.extend(sample)\n",
        "\n",
        "        # clean the data: tokenize each review\n",
        "        processed_data = []\n",
        "        for row in sampled_rows:\n",
        "            tokens = clean_text(row['text'])\n",
        "            title = row['title'].lower().strip()\n",
        "            # Handle None timestamp safely\n",
        "            try:\n",
        "                time = datetime.fromtimestamp(row['timestamp'] / 1000) if row['timestamp'] else None\n",
        "            except (TypeError, ValueError, OSError):\n",
        "                time = None\n",
        "            \n",
        "            # Determine sentiment label: 2=positive, 1=neutral, 0=negative\n",
        "            rating = row['rating']\n",
        "            if rating in [1, 2]:\n",
        "                sentiment = 2  # positive\n",
        "            elif rating == 3:\n",
        "                sentiment = 1  # neutral\n",
        "            elif rating in [4, 5]:\n",
        "                sentiment = 0  # negative\n",
        "            else:\n",
        "                sentiment = None\n",
        "\n",
        "            if tokens:\n",
        "                processed_data.append({\n",
        "                    'category': category,\n",
        "                    'tokens': tokens,\n",
        "                    'rating': row['rating'],\n",
        "                    'sentiment': sentiment,\n",
        "                    'original_text': row['text'],\n",
        "                    'title': title,\n",
        "                    'images': row['images'],\n",
        "                    'verified_purchase': row['verified_purchase'],\n",
        "                    'asin': row['asin'],\n",
        "                    'parent_asin': row['parent_asin'],\n",
        "                    'user_id': row['user_id'],\n",
        "                    'datetime': time,\n",
        "                    'helpful_vote': row['helpful_vote']\n",
        "                })\n",
        "\n",
        "        print(f\"{category}: {len(processed_data)} reviews processed\")\n",
        "        return category, processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{category}: Error - {str(e)}\")\n",
        "        return category, []\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GgmuYp6QF-T",
        "outputId": "45b8dbf0-0296-49c5-bbeb-bbb22731e89c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyFQQw0OPSps",
        "outputId": "f944a83e-76d4-468e-cb48-8659a491a914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appliances: 499 reviews processed\n",
            "Amazon_Fashion: 500 reviews processed\n",
            "All_Beauty: 500 reviews processed\n",
            "Arts_Crafts_and_Sewing: 499 reviews processed\n",
            "Automotive: 498 reviews processed\n",
            "Baby_Products: 499 reviews processed\n",
            "Beauty_and_Personal_Care: 500 reviews processed\n",
            "Books: 500 reviews processed\n",
            "CDs_and_Vinyl: 500 reviews processed\n",
            "Cell_Phones_and_Accessories: 494 reviews processed\n",
            "Digital_Music: 498 reviews processed\n",
            "Clothing_Shoes_and_Jewelry: 500 reviews processed\n",
            "Gift_Cards: 494 reviews processed\n",
            "Electronics: 494 reviews processed\n",
            "Grocery_and_Gourmet_Food: 500 reviews processed\n",
            "Handmade_Products: 499 reviews processed\n",
            "Health_and_Personal_Care: 500 reviews processed\n",
            "Health_and_Household: 500 reviews processed\n",
            "Home_and_Kitchen: 500 reviews processed\n",
            "Industrial_and_Scientific: 497 reviews processed\n",
            "Kindle_Store: 500 reviews processed\n",
            "Magazine_Subscriptions: 499 reviews processed\n",
            "Movies_and_TV: 500 reviews processed\n",
            "Musical_Instruments: 500 reviews processed\n",
            "Office_Products: 499 reviews processed\n",
            "Patio_Lawn_and_Garden: 497 reviews processed\n",
            "Pet_Supplies: 497 reviews processed\n",
            "Software: 495 reviews processed\n",
            "Subscription_Boxes: 500 reviews processed\n",
            "Sports_and_Outdoors: 500 reviews processed\n",
            "Tools_and_Home_Improvement: 496 reviews processed\n",
            "Toys_and_Games: 498 reviews processed\n",
            "Video_Games: 498 reviews processed\n",
            "Unknown: 499 reviews processed\n"
          ]
        }
      ],
      "source": [
        "# execute\n",
        "category_collection = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = {executor.submit(process_category, cat): cat for cat in categories}\n",
        "    for future in as_completed(futures):\n",
        "        cat, data_list = future.result()\n",
        "        category_collection[cat] = data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iV8M81CEQwT7"
      },
      "outputs": [],
      "source": [
        "final_list = []\n",
        "for cat, items in category_collection.items():\n",
        "    for item in items:\n",
        "        final_list.append({\n",
        "            'category': item['category'],\n",
        "            'rating': item['rating'],\n",
        "            'sentiment': item['sentiment'],\n",
        "            'tokens': item['tokens'],\n",
        "            'text_cleaned': ' '.join(item['tokens']),\n",
        "            'original_text': item['original_text'],\n",
        "            'token_count': len(item['tokens']),\n",
        "            'title': item['title'],\n",
        "            'images': item['images'],\n",
        "            'verified_purchase': item['verified_purchase'],\n",
        "            'asin': item['asin'],\n",
        "            'parent_asin': item['parent_asin'],\n",
        "            'user_id': item['user_id'],\n",
        "            'datetime': item['datetime'],\n",
        "            'helpful_vote': item['helpful_vote']\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(final_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "1RIps5wwQ8Y1",
        "outputId": "89a4b73e-b017-4bba-f45e-e8ed8f2a5ac3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 16949,\n  \"fields\": [\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 34,\n        \"samples\": [\n          \"Handmade_Products\",\n          \"Industrial_and_Scientific\",\n          \"Software\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.163135411806786,\n        \"min\": 1.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4.0,\n          3.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"positive\",\n          \"negative\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_cleaned\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16121,\n        \"samples\": [\n          \"perfect case iphone purchased amazon price fact know people love otterbox held well nice sleek well protect ipad\",\n          \"felt good skin put applied ageless added drop angel skin want make sure take advantage skin care get seems good start\",\n          \"family love time timer family thanks handful home one work use daily thought whiteboard version would great work since also use whiteboards help top br br weird setup whiteboard size sheet paper see photo chunk taken timer size whiteboard take quite bit writing space theory work well thing like pomodoro technique write task set timer writing possible also actual writing experience quite uncomfortable nice tray back keep marker eraser make writing whiteboard flat surface impossible angle whiteboard make wrist br br material wish whiteboards came sturdy one unfortunately removed timer thank goodness removable least put whiteboard desk\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 16486,\n        \"samples\": [\n          \"I love Charmin mega rolls toilet paper and I love home delivery.  This was a win/win for me.\",\n          \"Okay, so we may have burnt down our Sim house a few times and killed a few gnomes before we got the hang of it, but once we did, this game is really great. I will warn anyone who hasn't played it that it can become a bit addicting :) I had fun with the single player mode, but also with building a neighborhood and the kids loved it as well. If you are looking for a game that will keep you playing for a long time, this one should do the trick.\",\n          \"great I love Bruno Mars\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49,\n        \"min\": 1,\n        \"max\": 1029,\n        \"num_unique_values\": 365,\n        \"samples\": [\n          247,\n          43,\n          19\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13034,\n        \"samples\": [\n          \"i'm in love with the 3/4 inch sleeves and the irregular hemline\",\n          \"perfect on the go!\",\n          \"good challenge\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"images\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"verified_purchase\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"asin\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15793,\n        \"samples\": [\n          \"B004DLPXAO\",\n          \"B081JHH3V5\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"parent_asin\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15557,\n        \"samples\": [\n          \"B0C6CLBBQ1\",\n          \"B0BSG72K8B\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5810,\n        \"samples\": [\n          \"AFXQDJLOL5GVFJRHGTU2LNMZHEMA\",\n          \"AHS3BYEXVP4RMJMIDJUDKGFROX7Q\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"datetime\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1998-04-22 03:53:06\",\n        \"max\": \"2023-03-27 23:16:49.054000\",\n        \"num_unique_values\": 16948,\n        \"samples\": [\n          \"2019-06-18 14:59:10.766000\",\n          \"2019-08-10 01:23:42.902000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"helpful_vote\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 0,\n        \"max\": 929,\n        \"num_unique_values\": 107,\n        \"samples\": [\n          145,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f7346b55-543c-4f2a-bfc2-5aa7106a4b49\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>rating</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tokens</th>\n",
              "      <th>text_cleaned</th>\n",
              "      <th>original_text</th>\n",
              "      <th>token_count</th>\n",
              "      <th>title</th>\n",
              "      <th>images</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>asin</th>\n",
              "      <th>parent_asin</th>\n",
              "      <th>user_id</th>\n",
              "      <th>datetime</th>\n",
              "      <th>helpful_vote</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Appliances</td>\n",
              "      <td>5.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>[exelent, save, coffee, water, waste, easy, cl...</td>\n",
              "      <td>exelent save coffee water waste easy clean act...</td>\n",
              "      <td>Exelent,  save on coffee,  water,  no waste, e...</td>\n",
              "      <td>14</td>\n",
              "      <td>savings</td>\n",
              "      <td>[]</td>\n",
              "      <td>True</td>\n",
              "      <td>B00LGEKOMS</td>\n",
              "      <td>B07RNJY499</td>\n",
              "      <td>AEOVCZC77QZJQPBIAIKCFV7AS7PA</td>\n",
              "      <td>2017-10-16 22:46:40.529</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Appliances</td>\n",
              "      <td>5.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>[ordered, wrong, part, quality, part, seemed, ...</td>\n",
              "      <td>ordered wrong part quality part seemed good</td>\n",
              "      <td>Ordered wrong part but the quality of this par...</td>\n",
              "      <td>7</td>\n",
              "      <td>check part numbers</td>\n",
              "      <td>[]</td>\n",
              "      <td>True</td>\n",
              "      <td>B094YWPF68</td>\n",
              "      <td>B094YWPF68</td>\n",
              "      <td>AEU2V36H3G45EFVLASUPD56B7ATQ</td>\n",
              "      <td>2021-08-30 19:50:53.564</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Appliances</td>\n",
              "      <td>5.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>[described]</td>\n",
              "      <td>described</td>\n",
              "      <td>as described</td>\n",
              "      <td>1</td>\n",
              "      <td>five stars</td>\n",
              "      <td>[]</td>\n",
              "      <td>True</td>\n",
              "      <td>B00LQL043A</td>\n",
              "      <td>B00LQL043A</td>\n",
              "      <td>AE7FJMYY4AKWBDASLTMMQ5WASB7A</td>\n",
              "      <td>2016-09-28 15:08:18.000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Appliances</td>\n",
              "      <td>5.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>[used, replace, broken, door, bin, back, ice, ...</td>\n",
              "      <td>used replace broken door bin back ice dispense...</td>\n",
              "      <td>Used to replace a broken door bin on the back ...</td>\n",
              "      <td>18</td>\n",
              "      <td>fits</td>\n",
              "      <td>[]</td>\n",
              "      <td>True</td>\n",
              "      <td>B00C29G3N0</td>\n",
              "      <td>B00C29G3N0</td>\n",
              "      <td>AGM4WN3EOAA3RUAUXGH2S2AUL6WA</td>\n",
              "      <td>2019-01-06 21:01:13.614</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Appliances</td>\n",
              "      <td>5.0</td>\n",
              "      <td>positive</td>\n",
              "      <td>[husband, us, broke, cleaning, replace, well, ...</td>\n",
              "      <td>husband us broke cleaning replace well say one...</td>\n",
              "      <td>My husband uses these. I broke it when cleanin...</td>\n",
              "      <td>35</td>\n",
              "      <td>easy peasy to use and clean.</td>\n",
              "      <td>[]</td>\n",
              "      <td>True</td>\n",
              "      <td>B01DP1IWKU</td>\n",
              "      <td>B092LLM7H3</td>\n",
              "      <td>AEBXJRP4COCKP22LPKUDVCQ7JKVQ</td>\n",
              "      <td>2019-08-09 02:13:50.975</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7346b55-543c-4f2a-bfc2-5aa7106a4b49')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f7346b55-543c-4f2a-bfc2-5aa7106a4b49 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f7346b55-543c-4f2a-bfc2-5aa7106a4b49');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     category  rating sentiment  \\\n",
              "0  Appliances     5.0  positive   \n",
              "1  Appliances     5.0  positive   \n",
              "2  Appliances     5.0  positive   \n",
              "3  Appliances     5.0  positive   \n",
              "4  Appliances     5.0  positive   \n",
              "\n",
              "                                              tokens  \\\n",
              "0  [exelent, save, coffee, water, waste, easy, cl...   \n",
              "1  [ordered, wrong, part, quality, part, seemed, ...   \n",
              "2                                        [described]   \n",
              "3  [used, replace, broken, door, bin, back, ice, ...   \n",
              "4  [husband, us, broke, cleaning, replace, well, ...   \n",
              "\n",
              "                                        text_cleaned  \\\n",
              "0  exelent save coffee water waste easy clean act...   \n",
              "1        ordered wrong part quality part seemed good   \n",
              "2                                          described   \n",
              "3  used replace broken door bin back ice dispense...   \n",
              "4  husband us broke cleaning replace well say one...   \n",
              "\n",
              "                                       original_text  token_count  \\\n",
              "0  Exelent,  save on coffee,  water,  no waste, e...           14   \n",
              "1  Ordered wrong part but the quality of this par...            7   \n",
              "2                                       as described            1   \n",
              "3  Used to replace a broken door bin on the back ...           18   \n",
              "4  My husband uses these. I broke it when cleanin...           35   \n",
              "\n",
              "                          title images  verified_purchase        asin  \\\n",
              "0                       savings     []               True  B00LGEKOMS   \n",
              "1            check part numbers     []               True  B094YWPF68   \n",
              "2                    five stars     []               True  B00LQL043A   \n",
              "3                          fits     []               True  B00C29G3N0   \n",
              "4  easy peasy to use and clean.     []               True  B01DP1IWKU   \n",
              "\n",
              "  parent_asin                       user_id                datetime  \\\n",
              "0  B07RNJY499  AEOVCZC77QZJQPBIAIKCFV7AS7PA 2017-10-16 22:46:40.529   \n",
              "1  B094YWPF68  AEU2V36H3G45EFVLASUPD56B7ATQ 2021-08-30 19:50:53.564   \n",
              "2  B00LQL043A  AE7FJMYY4AKWBDASLTMMQ5WASB7A 2016-09-28 15:08:18.000   \n",
              "3  B00C29G3N0  AGM4WN3EOAA3RUAUXGH2S2AUL6WA 2019-01-06 21:01:13.614   \n",
              "4  B092LLM7H3  AEBXJRP4COCKP22LPKUDVCQ7JKVQ 2019-08-09 02:13:50.975   \n",
              "\n",
              "   helpful_vote  \n",
              "0             0  \n",
              "1             0  \n",
              "2             0  \n",
              "3             3  \n",
              "4             0  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3sE1wuERBzd"
      },
      "outputs": [],
      "source": [
        "df.to_parquet('amazon_user_reviews_3cat.parquet', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
