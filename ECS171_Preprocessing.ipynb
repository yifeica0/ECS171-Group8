{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://www.geeksforgeeks.org/nlp/amazon-product-reviews-sentiment-analysis-in-python/\n"
      ],
      "metadata": {
        "id": "C99Hul6-Lkf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BkWPBlb6jhN",
        "outputId": "16fe81bf-9e29-414e-c975-14bf62d0a10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import random\n",
        "import nltk\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize once for all processes\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jll8MTHuKXtu",
        "outputId": "0e5bfaac-898b-4f27-c60a-22faf2bb55ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    \"All_Beauty\",\n",
        "    \"Amazon_Fashion\",\n",
        "    \"Appliances\",\n",
        "    \"Arts_Crafts_and_Sewing\",\n",
        "    \"Automotive\",\n",
        "    \"Baby_Products\",\n",
        "    \"Beauty_and_Personal_Care\",\n",
        "    \"Books\",\n",
        "    \"CDs_and_Vinyl\",\n",
        "    \"Cell_Phones_and_Accessories\",\n",
        "    \"Clothing_Shoes_and_Jewelry\",\n",
        "    \"Digital_Music\",\n",
        "    \"Electronics\",\n",
        "    \"Gift_Cards\",\n",
        "    \"Grocery_and_Gourmet_Food\",\n",
        "    \"Handmade_Products\",\n",
        "    \"Health_and_Household\",\n",
        "    \"Health_and_Personal_Care\",\n",
        "    \"Home_and_Kitchen\",\n",
        "    \"Industrial_and_Scientific\",\n",
        "    \"Kindle_Store\",\n",
        "    \"Magazine_Subscriptions\",\n",
        "    \"Movies_and_TV\",\n",
        "    \"Musical_Instruments\",\n",
        "    \"Office_Products\",\n",
        "    \"Patio_Lawn_and_Garden\",\n",
        "    \"Pet_Supplies\",\n",
        "    \"Software\",\n",
        "    \"Sports_and_Outdoors\",\n",
        "    \"Subscription_Boxes\",\n",
        "    \"Tools_and_Home_Improvement\",\n",
        "    \"Toys_and_Games\",\n",
        "    \"Video_Games\",\n",
        "    \"Unknown\"\n",
        "]"
      ],
      "metadata": {
        "id": "RDFXNeaYKdEt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text data\n",
        "    - Convert to lowercase\n",
        "    - Tokenize into words\n",
        "    - Remove stopwords, punctuation, non-alphabetic tokens\n",
        "    - Lemmatize words\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(w)\n",
        "        for w in tokens\n",
        "        if w not in stop_words and w not in string.punctuation and w.isalpha()\n",
        "    ]\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "gmP6qZTGNDWn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data of all categories; store in the form that includes: category, reviews and sentiment level (1-5)\n",
        "\n",
        "# we load 5000 reviews from each category, then randomly sample 500 reviews for each category\n",
        "def process_category(category, sample_size=500, reservoir_size=5000):\n",
        "    \"\"\" return a dictionary with the structue: {\"category\":[\n",
        "      {\"category\":[name],\"tokens\":[......], \"rating\":[1]},{...}\n",
        "      ]}\"\"\"\n",
        "    try:\n",
        "\n",
        "      # load the data from hugging face\n",
        "        data_url = f\"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/review_categories/{category}.jsonl\"\n",
        "\n",
        "        dataset = load_dataset(\"json\", data_files=data_url, split=\"train\", streaming=True)\n",
        "\n",
        "        raw_rows = []\n",
        "        for i, row in enumerate(dataset):\n",
        "            if i >= reservoir_size:\n",
        "                break\n",
        "\n",
        "            raw_rows.append({\n",
        "                'text': row.get('text', ''),\n",
        "                'rating': row.get('rating', row.get('overall', None)),\n",
        "                'title': row.get('title', ''),\n",
        "                'images': row.get('images', []),\n",
        "                'verified_purchase': row.get('verified_purchase', False),\n",
        "                'asin': row.get('asin', ''),\n",
        "                'parent_asin': row.get('parent_asin', ''),\n",
        "                'user_id': row.get('user_id', ''),\n",
        "                'timestamp': row.get('timestamp', None),\n",
        "                'helpful_vote': row.get('helpful_vote', 0)\n",
        "            })\n",
        "\n",
        "        # set a seed for randomization\n",
        "        random.seed(42)\n",
        "        sampled_rows = random.sample(raw_rows, min(sample_size, len(raw_rows)))\n",
        "\n",
        "        # clean the data: tokenize each review\n",
        "        processed_data = []\n",
        "        for row in sampled_rows:\n",
        "            tokens = clean_text(row['text'])\n",
        "            title = row['title'].lower().strip()\n",
        "            time = datetime.fromtimestamp(row['timestamp']/ 1000)\n",
        "            if tokens:\n",
        "                processed_data.append({\n",
        "                    'category': category,\n",
        "                    'tokens': tokens,\n",
        "                    'rating': row['rating'],\n",
        "                    'original_text': row['text'],\n",
        "                    'title': title,\n",
        "                    'images': row['images'],\n",
        "                    'verified_purchase': row['verified_purchase'],\n",
        "                    'asin': row['asin'],\n",
        "                    'parent_asin': row['parent_asin'],\n",
        "                    'user_id': row['user_id'],\n",
        "                    'datetime': time,\n",
        "                    'helpful_vote': row['helpful_vote']\n",
        "                })\n",
        "\n",
        "        print(f\"{category}: {len(processed_data)} reviews processed\")\n",
        "        return category, processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{category}: Error - {str(e)}\")\n",
        "        return category, []\n",
        "#"
      ],
      "metadata": {
        "id": "x-VvhMtJK8JO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GgmuYp6QF-T",
        "outputId": "4007e559-1e77-42eb-cb8e-a8298e9d0cb6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# execute\n",
        "category_collection = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = {executor.submit(process_category, cat): cat for cat in categories}\n",
        "    for future in as_completed(futures):\n",
        "        cat, data_list = future.result()\n",
        "        category_collection[cat] = data_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyFQQw0OPSps",
        "outputId": "a1d659a6-3d99-4215-afe7-26283183ec7a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appliances: 499 reviews processed\n",
            "All_Beauty: 500 reviews processed\n",
            "Amazon_Fashion: Error - 'WordNetCorpusReader' object has no attribute '_LazyCorpusLoader__args'\n",
            "Arts_Crafts_and_Sewing: Error - 'WordNetCorpusReader' object has no attribute '_LazyCorpusLoader__args'\n",
            "Automotive: 498 reviews processed\n",
            "Baby_Products: 499 reviews processed\n",
            "Beauty_and_Personal_Care: 500 reviews processed\n",
            "Books: 500 reviews processed\n",
            "CDs_and_Vinyl: 500 reviews processed\n",
            "Cell_Phones_and_Accessories: 494 reviews processed\n",
            "Clothing_Shoes_and_Jewelry: 500 reviews processed\n",
            "Digital_Music: 498 reviews processed\n",
            "Electronics: 494 reviews processed\n",
            "Gift_Cards: 494 reviews processed\n",
            "Grocery_and_Gourmet_Food: 500 reviews processed\n",
            "Handmade_Products: 499 reviews processed\n",
            "Health_and_Household: 500 reviews processed\n",
            "Health_and_Personal_Care: 500 reviews processed\n",
            "Home_and_Kitchen: 500 reviews processed\n",
            "Industrial_and_Scientific: 497 reviews processed\n",
            "Kindle_Store: 500 reviews processed\n",
            "Magazine_Subscriptions: 499 reviews processed\n",
            "Movies_and_TV: 500 reviews processed\n",
            "Musical_Instruments: 500 reviews processed\n",
            "Office_Products: 499 reviews processed\n",
            "Patio_Lawn_and_Garden: 497 reviews processed\n",
            "Pet_Supplies: 497 reviews processed\n",
            "Software: 495 reviews processed\n",
            "Sports_and_Outdoors: 500 reviews processed\n",
            "Subscription_Boxes: 500 reviews processed\n",
            "Tools_and_Home_Improvement: 496 reviews processed\n",
            "Toys_and_Games: 498 reviews processed\n",
            "Video_Games: 498 reviews processed\n",
            "Unknown: 499 reviews processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_list = []\n",
        "for cat, items in category_collection.items():\n",
        "    for item in items:\n",
        "        final_list.append({\n",
        "            'category': item['category'],\n",
        "            'rating': item['rating'],\n",
        "            'tokens': item['tokens'],\n",
        "            'text_cleaned': ' '.join(item['tokens']),\n",
        "            'original_text': item['original_text'],\n",
        "            'token_count': len(item['tokens']),\n",
        "            'title': item['title'],\n",
        "            'images': item['images'],\n",
        "            'verified_purchase': item['verified_purchase'],\n",
        "            'asin': item['asin'],\n",
        "            'parent_asin': item['parent_asin'],\n",
        "            'user_id': item['user_id'],\n",
        "            'datetime': item['datetime'],\n",
        "            'helpful_vote': item['helpful_vote']\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(final_list)"
      ],
      "metadata": {
        "id": "iV8M81CEQwT7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "2b8a4978-cdda-4119-b03a-08661cd8afb3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'category_collection' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2086090513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfinal_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategory_collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         final_list.append({\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m'category'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'category_collection' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['category', 'rating', 'token_count', 'text_cleaned']].head())"
      ],
      "metadata": {
        "id": "1RIps5wwQ8Y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2e63c6-bddb-4359-f590-fbb984e30d1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     category  rating  token_count  \\\n",
            "0  Appliances     5.0           14   \n",
            "1  Appliances     5.0            7   \n",
            "2  Appliances     5.0            1   \n",
            "3  Appliances     5.0           18   \n",
            "4  Appliances     5.0           35   \n",
            "\n",
            "                                        text_cleaned  \n",
            "0  exelent save coffee water waste easy clean act...  \n",
            "1        ordered wrong part quality part seemed good  \n",
            "2                                          described  \n",
            "3  used replace broken door bin back ice dispense...  \n",
            "4  husband us broke cleaning replace well say one...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_parquet('amazon_user_reviews.parquet', index=False)"
      ],
      "metadata": {
        "id": "C3sE1wuERBzd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load meta data of all categories\n",
        "\n",
        "# we load 5000 reviews from each category, then randomly sample 500 reviews for each category\n",
        "def process_meta(category, sample_size=500, reservoir_size=5000):\n",
        "    \"\"\" return a dictionary with the structue: {\"category\":[\n",
        "      {\"category\":[name],\"tokens\":[......], \"rating\":[1]},{...}\n",
        "      ]}\"\"\"\n",
        "    try:\n",
        "\n",
        "      # load the data from hugging face\n",
        "        data_url = f\"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/meta_categories/meta_{category}.jsonl\"\n",
        "\n",
        "        dataset = load_dataset(\"json\", data_files=data_url, split=\"train\", streaming=True)\n",
        "\n",
        "        raw_rows = []\n",
        "        for i, row in enumerate(dataset):\n",
        "            if i >= reservoir_size:\n",
        "                break\n",
        "\n",
        "            raw_rows.append({\n",
        "                'main_category': category,\n",
        "                'title': row.get('title', ''),\n",
        "                'average_rating': row.get('average_rating', None),\n",
        "                'rating_number': row.get('rating_number', None),\n",
        "                'features': row.get('features', []),\n",
        "                'description': row.get('description', ''),\n",
        "                'price': row.get('price', None),\n",
        "                'images': row.get('images', []),\n",
        "                'videos': row.get('videos', []),\n",
        "                'store': row.get('store', ''),\n",
        "                'categories': row.get('categories', []),\n",
        "                'details': row.get('details', {}),\n",
        "                'parent_asin': row.get('parent_asin', ''),\n",
        "                'user_id': row.get('user_id', ''),\n",
        "                'boughts_together': row.get('boughts_together', [])\n",
        "            })\n",
        "\n",
        "        # set a seed for randomization\n",
        "        random.seed(42)\n",
        "        sampled_rows = random.sample(raw_rows, min(sample_size, len(raw_rows)))\n",
        "\n",
        "        # clean the data: tokenize each review\n",
        "        processed_data = []\n",
        "        for row in sampled_rows:\n",
        "          # Ensure description is a list of strings\n",
        "          description_raw = row.get('description', '')\n",
        "          if isinstance(description_raw, str):\n",
        "              description_list = [description_raw] if description_raw else []\n",
        "          elif isinstance(description_raw, list):\n",
        "              description_list = [d for d in description_raw if isinstance(d, str)]\n",
        "          else:\n",
        "              description_list = [] # Handle unexpected types\n",
        "\n",
        "          # Ensure features is a list of strings\n",
        "          features_raw = row.get('features', [])\n",
        "          if isinstance(features_raw, str):\n",
        "              features_list = [features_raw] if features_raw else []\n",
        "          elif isinstance(features_raw, list):\n",
        "              features_list = [f for f in features_raw if isinstance(f, str)]\n",
        "          else:\n",
        "              features_list = [] # Handle unexpected types\n",
        "\n",
        "          description = [d.lower().strip() for d in description_list]\n",
        "          description_cleaned = [\" \".join(clean_text(d)) for d in description_list]\n",
        "          features = [f.lower().strip() for f in features_list]\n",
        "          features_cleaned = [\" \".join(clean_text(f)) for f in features_list]\n",
        "          title = row['title'].lower().strip()\n",
        "          processed_data.append({\n",
        "              'main_category': row['main_category'],\n",
        "              'average_rating': row['average_rating'],\n",
        "              'rating_number': row['rating_number'],\n",
        "              'description': description,\n",
        "              'description_cleaned': description_cleaned,\n",
        "              'features': features,\n",
        "              'features_cleaned': features_cleaned,\n",
        "              'title': title,\n",
        "              'price': row['price'],\n",
        "              'images': row['images'],\n",
        "              'videos': row['videos'],\n",
        "              'store': row['store'],\n",
        "              'categories': row['categories'],\n",
        "              'details': row['details'],\n",
        "              'parent_asin': row['parent_asin'],\n",
        "              'user_id': row['user_id'],\n",
        "              'boughts_together': row['boughts_together']\n",
        "          })\n",
        "\n",
        "        print(f\"{category}: {len(processed_data)} meta reviews processed\")\n",
        "        return category, processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{category}: Error - {str(e)}\")\n",
        "        return category, []\n"
      ],
      "metadata": {
        "id": "AIP8_jrUlBvn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# execute meta data retrival\n",
        "meta_collection = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = {executor.submit(process_meta, cat): cat for cat in categories}\n",
        "    for future in as_completed(futures):\n",
        "        cat, data_list = future.result()\n",
        "        meta_collection[cat] = data_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbVtqh4YsLSg",
        "outputId": "76f1e99a-8d23-45dd-92c1-917c4cd0b357"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon_Fashion: 500 meta reviews processed\n",
            "All_Beauty: 500 meta reviews processed\n",
            "Appliances: 500 meta reviews processed\n",
            "Arts_Crafts_and_Sewing: 500 meta reviews processed\n",
            "Baby_Products: 500 meta reviews processed\n",
            "Automotive: 500 meta reviews processed\n",
            "Beauty_and_Personal_Care: 500 meta reviews processed\n",
            "CDs_and_Vinyl: 500 meta reviews processed\n",
            "Books: 500 meta reviews processed\n",
            "Digital_Music: 500 meta reviews processed\n",
            "Gift_Cards: 500 meta reviews processed\n",
            "Cell_Phones_and_Accessories: 500 meta reviews processed\n",
            "Clothing_Shoes_and_Jewelry: 500 meta reviews processed\n",
            "Handmade_Products: 500 meta reviews processed\n",
            "Electronics: 500 meta reviews processed\n",
            "Grocery_and_Gourmet_Food: 500 meta reviews processed\n",
            "Health_and_Personal_Care: 500 meta reviews processed\n",
            "Health_and_Household: 500 meta reviews processed\n",
            "Magazine_Subscriptions: 500 meta reviews processed\n",
            "Industrial_and_Scientific: 500 meta reviews processed\n",
            "Home_and_Kitchen: 500 meta reviews processed\n",
            "Kindle_Store: 500 meta reviews processed\n",
            "Musical_Instruments: 500 meta reviews processed\n",
            "Pet_Supplies: 500 meta reviews processed\n",
            "Office_Products: 500 meta reviews processed\n",
            "Patio_Lawn_and_Garden: 500 meta reviews processed\n",
            "Subscription_Boxes: 500 meta reviews processed\n",
            "Software: 500 meta reviews processed\n",
            "Sports_and_Outdoors: 500 meta reviews processed\n",
            "Tools_and_Home_Improvement: 500 meta reviews processed\n",
            "Toys_and_Games: 500 meta reviews processed\n",
            "Unknown: 500 meta reviews processed\n",
            "Video_Games: 500 meta reviews processed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Failed to load JSON from file 'hf://datasets/McAuley-Lab/Amazon-Reviews-2023@main/raw/meta_categories/meta_Movies_and_TV.jsonl' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column(/details/Producers) changed from array to string in row 9\n",
            "ERROR:datasets.packaged_modules.json.json:Failed to load JSON from file 'hf://datasets/McAuley-Lab/Amazon-Reviews-2023@main/raw/meta_categories/meta_Movies_and_TV.jsonl' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column(/details/Producers) changed from array to string in row 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movies_and_TV: Error - JSON parse error: Column(/details/Producers) changed from array to string in row 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_meta_list = []\n",
        "for cat, items in meta_collection.items():\n",
        "    for item in items:\n",
        "        final_meta_list.append({\n",
        "            'main_category': item['main_category'],\n",
        "            'title': item['title'],\n",
        "            'average_rating': item['average_rating'],\n",
        "            'rating_number': item['rating_number'],\n",
        "            'features': item['features'],\n",
        "            'features_cleaned': item['features_cleaned'],\n",
        "            'description': item['description'],\n",
        "            'description_cleaned': item['description_cleaned'],\n",
        "            'price': item['price'],\n",
        "            'images': item['images'],\n",
        "            'videos': item['videos'],\n",
        "            'store': item['store'],\n",
        "            'categories': item['categories'],\n",
        "            'details': item['details'],\n",
        "            'parent_asin': item['parent_asin'],\n",
        "            'user_id': item['user_id'],\n",
        "            'boughts_together': item['boughts_together']\n",
        "        })\n",
        "\n",
        "meta_df = pd.DataFrame(final_meta_list)"
      ],
      "metadata": {
        "id": "vaH7deNlseVJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df.to_parquet('amazon_metadata.parquet', index=False)"
      ],
      "metadata": {
        "id": "TppBXV4AH6k6"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}