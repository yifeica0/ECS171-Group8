{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://www.geeksforgeeks.org/nlp/amazon-product-reviews-sentiment-analysis-in-python/\n"
      ],
      "metadata": {
        "id": "C99Hul6-Lkf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BkWPBlb6jhN",
        "outputId": "b42640ed-7d01-4d18-9b0f-40f4a4f4bd28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import random\n",
        "import nltk\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize once for all processes\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jll8MTHuKXtu",
        "outputId": "80f23b74-1c8e-4666-ed7a-7e53b62486fc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    \"All_Beauty\",\n",
        "    \"Amazon_Fashion\",\n",
        "    \"Appliances\",\n",
        "    \"Arts_Crafts_and_Sewing\",\n",
        "    \"Automotive\",\n",
        "    \"Baby_Products\",\n",
        "    \"Beauty_and_Personal_Care\",\n",
        "    \"Books\",\n",
        "    \"CDs_and_Vinyl\",\n",
        "    \"Cell_Phones_and_Accessories\",\n",
        "    \"Clothing_Shoes_and_Jewelry\",\n",
        "    \"Digital_Music\",\n",
        "    \"Electronics\",\n",
        "    \"Gift_Cards\",\n",
        "    \"Grocery_and_Gourmet_Food\",\n",
        "    \"Handmade_Products\",\n",
        "    \"Health_and_Household\",\n",
        "    \"Health_and_Personal_Care\",\n",
        "    \"Home_and_Kitchen\",\n",
        "    \"Industrial_and_Scientific\",\n",
        "    \"Kindle_Store\",\n",
        "    \"Magazine_Subscriptions\",\n",
        "    \"Movies_and_TV\",\n",
        "    \"Musical_Instruments\",\n",
        "    \"Office_Products\",\n",
        "    \"Patio_Lawn_and_Garden\",\n",
        "    \"Pet_Supplies\",\n",
        "    \"Software\",\n",
        "    \"Sports_and_Outdoors\",\n",
        "    \"Subscription_Boxes\",\n",
        "    \"Tools_and_Home_Improvement\",\n",
        "    \"Toys_and_Games\",\n",
        "    \"Video_Games\",\n",
        "    \"Unknown\"\n",
        "]"
      ],
      "metadata": {
        "id": "RDFXNeaYKdEt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text data\n",
        "    - Convert to lowercase\n",
        "    - Tokenize into words\n",
        "    - Remove stopwords, punctuation, non-alphabetic tokens\n",
        "    - Lemmatize words\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(w)\n",
        "        for w in tokens\n",
        "        if w not in stop_words and w not in string.punctuation and w.isalpha()\n",
        "    ]\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "gmP6qZTGNDWn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data of all categories; store in the form that includes: category, reviews and sentiment level (1-5)\n",
        "\n",
        "# we load 5000 reviews from each category, then randomly sample 500 reviews for each category\n",
        "def process_category(category, sample_size=500, reservoir_size=5000):\n",
        "    \"\"\" return a dictionary with the structue: {\"category\":[\n",
        "      {\"category\":[name],\"tokens\":[......], \"rating\":[1]},{...}\n",
        "      ]}\"\"\"\n",
        "    try:\n",
        "\n",
        "      # load the data from hugging face\n",
        "        data_url = f\"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/review_categories/{category}.jsonl\"\n",
        "\n",
        "        dataset = load_dataset(\"json\", data_files=data_url, split=\"train\", streaming=True)\n",
        "\n",
        "        raw_rows = []\n",
        "        for i, row in enumerate(dataset):\n",
        "            if i >= reservoir_size:\n",
        "                break\n",
        "\n",
        "            raw_rows.append({\n",
        "                'text': row.get('text', ''),\n",
        "                'rating': row.get('rating', row.get('overall', None)),\n",
        "                'title': row.get('title', ''),\n",
        "                'images': row.get('images', []),\n",
        "                'verified_purchase': row.get('verified_purchase', False),\n",
        "                'asin': row.get('asin', ''),\n",
        "                'parent_asin': row.get('parent_asin', ''),\n",
        "                'user_id': row.get('user_id', ''),\n",
        "                'timestamp': row.get('timestamp', None),\n",
        "                'helpful_vote': row.get('helpful_vote', 0)\n",
        "            })\n",
        "\n",
        "        # set a seed for randomization\n",
        "        random.seed(42)\n",
        "        sampled_rows = random.sample(raw_rows, min(sample_size, len(raw_rows)))\n",
        "\n",
        "        # clean the data: tokenize each review\n",
        "        processed_data = []\n",
        "        for row in sampled_rows:\n",
        "            tokens = clean_text(row['text'])\n",
        "            title = row['title'].lower().strip()\n",
        "            time = datetime.fromtimestamp(row['timestamp']/ 1000)\n",
        "            if tokens:\n",
        "                processed_data.append({\n",
        "                    'category': category,\n",
        "                    'tokens': tokens,\n",
        "                    'rating': row['rating'],\n",
        "                    'original_text': row['text'],\n",
        "                    'title': title,\n",
        "                    'images': row['images'],\n",
        "                    'verified_purchase': row['verified_purchase'],\n",
        "                    'asin': row['asin'],\n",
        "                    'parent_asin': row['parent_asin'],\n",
        "                    'user_id': row['user_id'],\n",
        "                    'datetime': time,\n",
        "                    'helpful_vote': row['helpful_vote']\n",
        "                })\n",
        "\n",
        "        print(f\"{category}: {len(processed_data)} reviews processed\")\n",
        "        return category, processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{category}: Error - {str(e)}\")\n",
        "        return category, []\n",
        "#"
      ],
      "metadata": {
        "id": "x-VvhMtJK8JO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GgmuYp6QF-T",
        "outputId": "95f2a26a-dd31-4447-972d-ddc2e8ff40fc"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# execute\n",
        "category_collection = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = {executor.submit(process_category, cat): cat for cat in categories}\n",
        "    for future in as_completed(futures):\n",
        "        cat, data_list = future.result()\n",
        "        category_collection[cat] = data_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyFQQw0OPSps",
        "outputId": "9a0bae14-a1fe-4dd5-82fb-19a6d47b08d9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All_Beauty: 500 reviews processed\n",
            "Amazon_Fashion: 500 reviews processed\n",
            "Appliances: 499 reviews processed\n",
            "Arts_Crafts_and_Sewing: 499 reviews processed\n",
            "Automotive: 498 reviews processed\n",
            "Baby_Products: 499 reviews processed\n",
            "Beauty_and_Personal_Care: 500 reviews processed\n",
            "CDs_and_Vinyl: 500 reviews processed\n",
            "Books: 500 reviews processed\n",
            "Cell_Phones_and_Accessories: 494 reviews processed\n",
            "Clothing_Shoes_and_Jewelry: 500 reviews processed\n",
            "Digital_Music: 498 reviews processed\n",
            "Electronics: 494 reviews processed\n",
            "Gift_Cards: 494 reviews processed\n",
            "Grocery_and_Gourmet_Food: 500 reviews processed\n",
            "Health_and_Household: 500 reviews processed\n",
            "Handmade_Products: 499 reviews processed\n",
            "Health_and_Personal_Care: 500 reviews processed\n",
            "Home_and_Kitchen: 500 reviews processed\n",
            "Industrial_and_Scientific: 497 reviews processed\n",
            "Magazine_Subscriptions: 499 reviews processed\n",
            "Kindle_Store: 500 reviews processed\n",
            "Movies_and_TV: 500 reviews processed\n",
            "Musical_Instruments: 500 reviews processed\n",
            "Office_Products: 499 reviews processed\n",
            "Patio_Lawn_and_Garden: 497 reviews processed\n",
            "Pet_Supplies: 497 reviews processed\n",
            "Software: 495 reviews processed\n",
            "Subscription_Boxes: 500 reviews processed\n",
            "Sports_and_Outdoors: 500 reviews processed\n",
            "Tools_and_Home_Improvement: 496 reviews processed\n",
            "Toys_and_Games: 498 reviews processed\n",
            "Video_Games: 498 reviews processed\n",
            "Unknown: 499 reviews processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_list = []\n",
        "for cat, items in category_collection.items():\n",
        "    for item in items:\n",
        "        final_list.append({\n",
        "            'category': item['category'],\n",
        "            'rating': item['rating'],\n",
        "            'tokens': item['tokens'],\n",
        "            'text_cleaned': ' '.join(item['tokens']),\n",
        "            'original_text': item['original_text'],\n",
        "            'token_count': len(item['tokens']),\n",
        "            'title': item['title'],\n",
        "            'images': item['images'],\n",
        "            'verified_purchase': item['verified_purchase'],\n",
        "            'asin': item['asin'],\n",
        "            'parent_asin': item['parent_asin'],\n",
        "            'user_id': item['user_id'],\n",
        "            'datetime': item['datetime'],\n",
        "            'helpful_vote': item['helpful_vote']\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(final_list)"
      ],
      "metadata": {
        "id": "iV8M81CEQwT7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['category', 'rating', 'token_count', 'text_cleaned']].head())"
      ],
      "metadata": {
        "id": "1RIps5wwQ8Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_parquet('amazon_user_reviews.parquet', index=False)"
      ],
      "metadata": {
        "id": "C3sE1wuERBzd"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}