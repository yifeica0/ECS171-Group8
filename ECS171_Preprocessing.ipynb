{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://www.geeksforgeeks.org/nlp/amazon-product-reviews-sentiment-analysis-in-python/\n"
      ],
      "metadata": {
        "id": "C99Hul6-Lkf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BkWPBlb6jhN",
        "outputId": "37fdfdec-18c5-4a7d-c92e-b694b67c044f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import random\n",
        "import nltk\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize once for all processes\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jll8MTHuKXtu",
        "outputId": "f06fdba2-a5e5-4de4-fb71-d2d28bc348c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    \"All_Beauty\",\n",
        "    \"Amazon_Fashion\",\n",
        "    \"Appliances\",\n",
        "    \"Arts_Crafts_and_Sewing\",\n",
        "    \"Automotive\",\n",
        "    \"Baby_Products\",\n",
        "    \"Beauty_and_Personal_Care\",\n",
        "    \"Books\",\n",
        "    \"CDs_and_Vinyl\",\n",
        "    \"Cell_Phones_and_Accessories\",\n",
        "    \"Clothing_Shoes_and_Jewelry\",\n",
        "    \"Digital_Music\",\n",
        "    \"Electronics\",\n",
        "    \"Gift_Cards\",\n",
        "    \"Grocery_and_Gourmet_Food\",\n",
        "    \"Handmade_Products\",\n",
        "    \"Health_and_Household\",\n",
        "    \"Health_and_Personal_Care\",\n",
        "    \"Home_and_Kitchen\",\n",
        "    \"Industrial_and_Scientific\",\n",
        "    \"Kindle_Store\",\n",
        "    \"Magazine_Subscriptions\",\n",
        "    \"Movies_and_TV\",\n",
        "    \"Musical_Instruments\",\n",
        "    \"Office_Products\",\n",
        "    \"Patio_Lawn_and_Garden\",\n",
        "    \"Pet_Supplies\",\n",
        "    \"Software\",\n",
        "    \"Sports_and_Outdoors\",\n",
        "    \"Subscription_Boxes\",\n",
        "    \"Tools_and_Home_Improvement\",\n",
        "    \"Toys_and_Games\",\n",
        "    \"Video_Games\",\n",
        "    \"Unknown\"\n",
        "]"
      ],
      "metadata": {
        "id": "RDFXNeaYKdEt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text data\n",
        "    - Convert to lowercase\n",
        "    - Tokenize into words\n",
        "    - Remove stopwords, punctuation, non-alphabetic tokens\n",
        "    - Lemmatize words\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(w)\n",
        "        for w in tokens\n",
        "        if w not in stop_words and w not in string.punctuation and w.isalpha()\n",
        "    ]\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "gmP6qZTGNDWn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data of all categories; store in the form that includes: category, reviews and sentiment level (1-5)\n",
        "\n",
        "# we load 5000 reviews from each category, then randomly sample 500 reviews for each category\n",
        "def process_category(category, sample_size=500, reservoir_size=5000):\n",
        "    \"\"\" return a dictionary with the structue: {\"category\":[\n",
        "      {\"category\":[name],\"tokens\":[......], \"rating\":[1]},{...}\n",
        "      ]}\"\"\"\n",
        "    try:\n",
        "\n",
        "      # load the data from hugging face\n",
        "        data_url = f\"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/review_categories/{category}.jsonl\"\n",
        "\n",
        "        dataset = load_dataset(\"json\", data_files=data_url, split=\"train\", streaming=True)\n",
        "\n",
        "        raw_rows = []\n",
        "        for i, row in enumerate(dataset):\n",
        "            if i >= reservoir_size:\n",
        "                break\n",
        "\n",
        "            raw_rows.append({\n",
        "                'text': row.get('text', ''),\n",
        "                'rating': row.get('rating', row.get('overall', None))\n",
        "            })\n",
        "\n",
        "        # set a seed for randomization\n",
        "        random.seed(42)\n",
        "        sampled_rows = random.sample(raw_rows, min(sample_size, len(raw_rows)))\n",
        "\n",
        "        # clean the data: tokenize each review\n",
        "        processed_data = []\n",
        "        for row in sampled_rows:\n",
        "            tokens = clean_text(row['text'])\n",
        "            if tokens:\n",
        "                processed_data.append({\n",
        "                    'category': category,\n",
        "                    'tokens': tokens,\n",
        "                    'rating': row['rating'],\n",
        "                    'original_text': row['text']\n",
        "                })\n",
        "\n",
        "        print(f\"{category}: {len(processed_data)} reviews processed\")\n",
        "        return category, processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{category}: Error - {str(e)}\")\n",
        "        return category, []\n",
        "#"
      ],
      "metadata": {
        "id": "x-VvhMtJK8JO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GgmuYp6QF-T",
        "outputId": "eaa4a105-c9fe-4167-878d-a3bb62be4867"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# execute\n",
        "category_collection = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = {executor.submit(process_category, cat): cat for cat in categories}\n",
        "    for future in as_completed(futures):\n",
        "        cat, data_list = future.result()\n",
        "        category_collection[cat] = data_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyFQQw0OPSps",
        "outputId": "29228caf-4e73-4f6f-982f-0af26f903eb7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amazon_Fashion: 500 reviews processed\n",
            "Arts_Crafts_and_Sewing: 499 reviews processed\n",
            "Appliances: 499 reviews processed\n",
            "All_Beauty: 500 reviews processed\n",
            "Automotive: 498 reviews processed\n",
            "Baby_Products: 499 reviews processed\n",
            "Beauty_and_Personal_Care: 500 reviews processed\n",
            "Books: 500 reviews processed\n",
            "CDs_and_Vinyl: 500 reviews processed\n",
            "Cell_Phones_and_Accessories: 494 reviews processed\n",
            "Clothing_Shoes_and_Jewelry: 500 reviews processed\n",
            "Digital_Music: 498 reviews processed\n",
            "Electronics: 494 reviews processed\n",
            "Gift_Cards: 494 reviews processed\n",
            "Grocery_and_Gourmet_Food: 500 reviews processed\n",
            "Handmade_Products: 499 reviews processed\n",
            "Health_and_Household: 500 reviews processed\n",
            "Health_and_Personal_Care: 500 reviews processed\n",
            "Home_and_Kitchen: 500 reviews processed\n",
            "Industrial_and_Scientific: 497 reviews processed\n",
            "Magazine_Subscriptions: 499 reviews processed\n",
            "Kindle_Store: 500 reviews processed\n",
            "Movies_and_TV: 500 reviews processed\n",
            "Musical_Instruments: 500 reviews processed\n",
            "Office_Products: 499 reviews processed\n",
            "Patio_Lawn_and_Garden: 497 reviews processed\n",
            "Pet_Supplies: 497 reviews processed\n",
            "Software: 495 reviews processed\n",
            "Sports_and_Outdoors: 500 reviews processed\n",
            "Subscription_Boxes: 500 reviews processed\n",
            "Tools_and_Home_Improvement: 496 reviews processed\n",
            "Toys_and_Games: 498 reviews processed\n",
            "Video_Games: 498 reviews processed\n",
            "Unknown: 499 reviews processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_list = []\n",
        "for cat, items in category_collection.items():\n",
        "    for item in items:\n",
        "        final_list.append({\n",
        "            'category': item['category'],\n",
        "            'rating': item['rating'],\n",
        "            'tokens': item['tokens'],\n",
        "            'text_cleaned': ' '.join(item['tokens']),\n",
        "            'original_text': item['original_text'],\n",
        "            'token_count': len(item['tokens'])\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(final_list)"
      ],
      "metadata": {
        "id": "iV8M81CEQwT7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['category', 'rating', 'token_count', 'text_cleaned']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RIps5wwQ8Y1",
        "outputId": "2ac12bbd-422c-471d-9da5-0cec3f35abc2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         category  rating  token_count  \\\n",
            "0  Amazon_Fashion     5.0           52   \n",
            "1  Amazon_Fashion     5.0           49   \n",
            "2  Amazon_Fashion     3.0           59   \n",
            "3  Amazon_Fashion     5.0          188   \n",
            "4  Amazon_Fashion     4.0            2   \n",
            "\n",
            "                                        text_cleaned  \n",
            "0  love maxi dress easy dressing throw one look g...  \n",
            "1  tend stay away sleeveless top plus size woman ...  \n",
            "2  thing complain sunglass organizer reason smell...  \n",
            "3  like big sunglass purple would really notice p...  \n",
            "4                                           nice bag  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_parquet('amazon_reviews_with_rating.parquet', index=False)"
      ],
      "metadata": {
        "id": "C3sE1wuERBzd"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}