{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://www.geeksforgeeks.org/nlp/amazon-product-reviews-sentiment-analysis-in-python/\n"
      ],
      "metadata": {
        "id": "C99Hul6-Lkf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BkWPBlb6jhN",
        "outputId": "44b5995a-a63d-4bde-bf55-8cbb6ecea158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import random\n",
        "import nltk\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "# Download required NLTK data (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize once for all processes\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jll8MTHuKXtu",
        "outputId": "a2928242-1761-41bf-c0c8-74c6898eb0fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    \"All_Beauty\",\n",
        "    \"Amazon_Fashion\",\n",
        "    \"Appliances\",\n",
        "    \"Arts_Crafts_and_Sewing\",\n",
        "    \"Automotive\",\n",
        "    \"Baby_Products\",\n",
        "    \"Beauty_and_Personal_Care\",\n",
        "    \"Books\",\n",
        "    \"CDs_and_Vinyl\",\n",
        "    \"Cell_Phones_and_Accessories\",\n",
        "    \"Clothing_Shoes_and_Jewelry\",\n",
        "    \"Digital_Music\",\n",
        "    \"Electronics\",\n",
        "    \"Gift_Cards\",\n",
        "    \"Grocery_and_Gourmet_Food\",\n",
        "    \"Handmade_Products\",\n",
        "    \"Health_and_Household\",\n",
        "    \"Health_and_Personal_Care\",\n",
        "    \"Home_and_Kitchen\",\n",
        "    \"Industrial_and_Scientific\",\n",
        "    \"Kindle_Store\",\n",
        "    \"Magazine_Subscriptions\",\n",
        "    \"Movies_and_TV\",\n",
        "    \"Musical_Instruments\",\n",
        "    \"Office_Products\",\n",
        "    \"Patio_Lawn_and_Garden\",\n",
        "    \"Pet_Supplies\",\n",
        "    \"Software\",\n",
        "    \"Sports_and_Outdoors\",\n",
        "    \"Subscription_Boxes\",\n",
        "    \"Tools_and_Home_Improvement\",\n",
        "    \"Toys_and_Games\",\n",
        "    \"Video_Games\",\n",
        "    \"Unknown\"\n",
        "]"
      ],
      "metadata": {
        "id": "RDFXNeaYKdEt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text data\n",
        "    - Convert to lowercase\n",
        "    - Tokenize into words\n",
        "    - Remove stopwords, punctuation, non-alphabetic tokens\n",
        "    - Lemmatize words\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    cleaned_tokens = [\n",
        "        lemmatizer.lemmatize(w)\n",
        "        for w in tokens\n",
        "        if w not in stop_words and w not in string.punctuation and w.isalpha()\n",
        "    ]\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "gmP6qZTGNDWn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data of all categories; store in the form that includes: category, reviews and sentiment level (1-5)\n",
        "\n",
        "# we load 5000 reviews from each category, then randomly sample 500 reviews for each category\n",
        "def process_category(category, sample_size=500, reservoir_size=5000):\n",
        "    \"\"\" return a dictionary with the structue: {\"category\":[\n",
        "      {\"category\":[name],\"tokens\":[......], \"rating\":[1]},{...}\n",
        "      ]}\"\"\"\n",
        "    try:\n",
        "\n",
        "      # load the data from hugging face\n",
        "        data_url = f\"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/review_categories/{category}.jsonl\"\n",
        "\n",
        "        dataset = load_dataset(\"json\", data_files=data_url, split=\"train\", streaming=True)\n",
        "\n",
        "        raw_rows = []\n",
        "        for i, row in enumerate(dataset):\n",
        "            if i >= reservoir_size:\n",
        "                break\n",
        "\n",
        "            raw_rows.append({\n",
        "                'text': row.get('text', ''),\n",
        "                'rating': row.get('rating', row.get('overall', None)),\n",
        "                'title': row.get('title', ''),\n",
        "                'images': row.get('images', []),\n",
        "                'verified_purchase': row.get('verified_purchase', False),\n",
        "                'asin': row.get('asin', ''),\n",
        "                'parent_asin': row.get('parent_asin', ''),\n",
        "                'user_id': row.get('user_id', ''),\n",
        "                'timestamp': row.get('timestamp', ''),\n",
        "                'helpful_vote': row.get('helpful_vote', '')\n",
        "            })\n",
        "\n",
        "        # set a seed for randomization\n",
        "        random.seed(42)\n",
        "        sampled_rows = random.sample(raw_rows, min(sample_size, len(raw_rows)))\n",
        "\n",
        "        # clean the data: tokenize each review\n",
        "        processed_data = []\n",
        "        for row in sampled_rows:\n",
        "            tokens = clean_text(row['text'])\n",
        "            title = row['title'].lower().strip()\n",
        "            if tokens:\n",
        "                processed_data.append({\n",
        "                    'category': category,\n",
        "                    'tokens': tokens,\n",
        "                    'rating': row['rating'],\n",
        "                    'original_text': row['text'],\n",
        "                    'title': title,\n",
        "                    'images': row['images'],\n",
        "                    'verified_purchase': row['verified_purchase'],\n",
        "                    'asin': row['asin'],\n",
        "                    'parent_asin': row['parent_asin'],\n",
        "                    'user_id': row['user_id'],\n",
        "                    'timestamp': row['timestamp'],\n",
        "                    'helpful_vote': row['helpful_vote']\n",
        "                })\n",
        "\n",
        "        print(f\"{category}: {len(processed_data)} reviews processed\")\n",
        "        return category, processed_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{category}: Error - {str(e)}\")\n",
        "        return category, []\n",
        "#"
      ],
      "metadata": {
        "id": "x-VvhMtJK8JO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GgmuYp6QF-T",
        "outputId": "ea5a3b40-a022-425d-fa40-2711410530c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# execute\n",
        "category_collection = {}\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = {executor.submit(process_category, cat): cat for cat in categories}\n",
        "    for future in as_completed(futures):\n",
        "        cat, data_list = future.result()\n",
        "        category_collection[cat] = data_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyFQQw0OPSps",
        "outputId": "dc2f07e0-894d-49a1-86ba-44deba6f191c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arts_Crafts_and_Sewing: 499 reviews processed\n",
            "Appliances: 499 reviews processed\n",
            "All_Beauty: 500 reviews processed\n",
            "Amazon_Fashion: Error - 'WordNetCorpusReader' object has no attribute '_LazyCorpusLoader__args'\n",
            "Automotive: 498 reviews processed\n",
            "Baby_Products: 499 reviews processed\n",
            "Beauty_and_Personal_Care: 500 reviews processed\n",
            "Cell_Phones_and_Accessories: 494 reviews processed\n",
            "Books: 500 reviews processed\n",
            "CDs_and_Vinyl: 500 reviews processed\n",
            "Clothing_Shoes_and_Jewelry: 500 reviews processed\n",
            "Gift_Cards: 494 reviews processed\n",
            "Digital_Music: 498 reviews processed\n",
            "Electronics: 494 reviews processed\n",
            "Grocery_and_Gourmet_Food: 500 reviews processed\n",
            "Handmade_Products: 499 reviews processed\n",
            "Health_and_Household: 500 reviews processed\n",
            "Health_and_Personal_Care: 500 reviews processed\n",
            "Industrial_and_Scientific: 497 reviews processed\n",
            "Home_and_Kitchen: 500 reviews processed\n",
            "Kindle_Store: 500 reviews processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_list = []\n",
        "for cat, items in category_collection.items():\n",
        "    for item in items:\n",
        "        final_list.append({\n",
        "            'category': item['category'],\n",
        "            'rating': item['rating'],\n",
        "            'tokens': item['tokens'],\n",
        "            'text_cleaned': ' '.join(item['tokens']),\n",
        "            'original_text': item['original_text'],\n",
        "            'token_count': len(item['tokens']),\n",
        "            'title': item['title'],\n",
        "            'images': item['images'],\n",
        "            'verified_purchase': item['verified_purchase'],\n",
        "            'asin': item['asin'],\n",
        "            'parent_asin': item['parent_asin'],\n",
        "            'user_id': item['user_id'],\n",
        "            'timestamp': item['timestamp'],\n",
        "            'helpful_vote': item['helpful_vote']\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(final_list)"
      ],
      "metadata": {
        "id": "iV8M81CEQwT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['category', 'rating', 'token_count', 'text_cleaned']].head())"
      ],
      "metadata": {
        "id": "1RIps5wwQ8Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_parquet('amazon_user_reviews.parquet', index=False)"
      ],
      "metadata": {
        "id": "C3sE1wuERBzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}